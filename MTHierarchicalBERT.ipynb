{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class WordAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(WordAttention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        attention_weights = torch.softmax(self.attention(word_embeddings), dim=0)\n",
    "        weighted_sum = torch.sum(attention_weights * word_embeddings, dim=0)\n",
    "        return weighted_sum  # Shape: [hidden_dim]\n",
    "\n",
    "class SentenceAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SentenceAttention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, sentence_embeddings):\n",
    "        attention_weights = torch.softmax(self.attention(sentence_embeddings), dim=0)\n",
    "        weighted_sum = torch.sum(attention_weights * sentence_embeddings, dim=0)\n",
    "        return weighted_sum  # Shape: [hidden_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskHierarchicalBERT(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, num_classes_sentiment=3, num_classes_emotion=5):\n",
    "        super(MultiTaskHierarchicalBERT, self).__init__()\n",
    "        self.word_attention = WordAttention(hidden_dim)\n",
    "        self.sentence_attention = SentenceAttention(hidden_dim)\n",
    "        \n",
    "        # Task-specific classifiers\n",
    "        self.sentiment_head = nn.Linear(hidden_dim, num_classes_sentiment)\n",
    "        self.emotion_head = nn.Linear(hidden_dim, num_classes_emotion)\n",
    "\n",
    "    def forward(self, documents):\n",
    "        sentence_embeddings = []\n",
    "        for doc in documents:  # Process each document\n",
    "            word_embeddings = encode_sentence(doc)\n",
    "            sentence_embedding = self.word_attention(word_embeddings)\n",
    "            sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "        sentence_embeddings = torch.stack(sentence_embeddings)\n",
    "        document_embedding = self.sentence_attention(sentence_embeddings)\n",
    "\n",
    "        # Task-specific outputs\n",
    "        sentiment_output = self.sentiment_head(document_embedding)\n",
    "        emotion_output = self.emotion_head(document_embedding)\n",
    "\n",
    "        return sentiment_output, emotion_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Same loss for both tasks (classification)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for documents, sentiment_labels, emotion_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        sentiment_output, emotion_output = model(documents)\n",
    "        \n",
    "        # Compute individual losses\n",
    "        sentiment_loss = criterion(sentiment_output, sentiment_labels)\n",
    "        emotion_loss = criterion(emotion_output, emotion_labels)\n",
    "        \n",
    "        # Combine losses (weighted sum)\n",
    "        total_loss = sentiment_loss + emotion_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0.7 * sentiment_loss + 0.3 * emotion_loss  # Weighted sum\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
