{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slang dataset\n",
    "\n",
    "https://huggingface.co/datasets/MLBtrio/genz-slang-dataset/viewer/default/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "slang_dataset = load_dataset(\"MLBtrio/genz-slang-dataset\")\n",
    "slang_words = [entry['Slang'] for entry in slang_dataset['train'] if ' ' not in entry['Slang'] and entry['Slang'].isalpha()]\n",
    "slang_words_set = set(word.lower() for word in slang_words)\n",
    "slang_words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- awww, that's a bummer.  you shoulda got davi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i dived many times for the ball. managed to sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196816</th>\n",
       "      <td>best viet hoagies you'll find in the area, or ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196817</th>\n",
       "      <td>if you need medical testing of any kind, i wou...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196818</th>\n",
       "      <td>this place is a dream. honestly my favorite in...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196819</th>\n",
       "      <td>great place to have your dog groom. my one dog...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196820</th>\n",
       "      <td>this salon is great! the pedicure was great, g...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yelp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2196821 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  polarity   source\n",
       "0        - awww, that's a bummer.  you shoulda got davi...         0  Twitter\n",
       "1        is upset that he can't update his facebook by ...         0  Twitter\n",
       "2        i dived many times for the ball. managed to sa...         0  Twitter\n",
       "3           my whole body feels itchy and like its on fire         0  Twitter\n",
       "4        no, it's not behaving at all. i'm mad. why am ...         0  Twitter\n",
       "...                                                    ...       ...      ...\n",
       "2196816  best viet hoagies you'll find in the area, or ...         1     Yelp\n",
       "2196817  if you need medical testing of any kind, i wou...         1     Yelp\n",
       "2196818  this place is a dream. honestly my favorite in...         1     Yelp\n",
       "2196819  great place to have your dog groom. my one dog...         1     Yelp\n",
       "2196820  this salon is great! the pedicure was great, g...         1     Yelp\n",
       "\n",
       "[2196821 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Datasets/Cleaned with tokens/combined_dataset.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slang_tokenizer(text):\n",
    "    tokens = word_tokenize(text.lower(), language='english', preserve_line=True)\n",
    "    return [word for word in tokens if word.isalpha() and word in slang_words_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=slang_tokenizer, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df.index = data.index\n",
    "\n",
    "# Aggregate TF-IDF scores for each slang term\n",
    "slang_scores = tfidf_df.sum(axis=0).sort_values(ascending=False)\n",
    "slang_scores.to_csv('slang_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Test the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "words = [\"u\", \"im\", \"wat\", 'lmao', 'lol', 'brb', 'omg', 'wtf', 'smh', 'idk', 'tbh', 'sry']\n",
    "for word in words:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(word)\n",
    "    if token_id == 100:  # ID 100 corresponds to the [UNK] token\n",
    "        print(f\"'{word}' is NOT in the BERT vocabulary (mapped to [UNK]).\")\n",
    "    else:\n",
    "        print(f\"'{word}' is in the BERT vocabulary with ID {token_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for example usage of the slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserting slang into BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f4322c2d90>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "TORCH_SEED = 42\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "torch.manual_seed(TORCH_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors from GloVe.\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    \n",
    "    print(f\"Loaded {len(embeddings_index)} word vectors from GloVe.\")\n",
    "    return embeddings_index\n",
    "\n",
    "file_path = 'Embeddings/glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "glove_embeddings = load_glove_embeddings(file_path)\n",
    "GLOVE_EMBEDDING_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_words = [\"lol\", \"lmao\", \"omg\", \"wtf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 'lol' with projected GloVe weights.\n",
      "Initialized 'lmao' with projected GloVe weights.\n",
      "Initialized 'omg' with projected GloVe weights.\n",
      "Initialized 'wtf' with projected GloVe weights.\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.bert.embeddings.word_embeddings\n",
    "\n",
    "projection_layer = torch.nn.Linear(GLOVE_EMBEDDING_SIZE, model.config.hidden_size)\n",
    "\n",
    "new_tokens = [token for token in slang_words if token not in tokenizer.get_vocab()]\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "embedding_layer = model.bert.embeddings.word_embeddings\n",
    "\n",
    "for slang in new_tokens:\n",
    "    token_index = tokenizer.convert_tokens_to_ids(slang)\n",
    "    \n",
    "    if slang in glove_embeddings:\n",
    "        glove_vector = torch.tensor(glove_embeddings[slang], dtype=torch.float32)\n",
    "        projected_vector = projection_layer(glove_vector.unsqueeze(0)).squeeze(0)\n",
    "        embedding_layer.weight.data[token_index] = projected_vector\n",
    "        print(f\"Initialized '{slang}' with projected GloVe weights.\")\n",
    "    else:\n",
    "        embedding_layer.weight.data[token_index] = torch.randn(model.config.hidden_size)\n",
    "        print(f\"Initialized '{slang}' with random weights.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'lol' successfully added to the tokenizer with index 30522.\n",
      "'lmao' successfully added to the tokenizer with index 30523.\n",
      "'omg' successfully added to the tokenizer with index 30524.\n",
      "'wtf' successfully added to the tokenizer with index 30525.\n"
     ]
    }
   ],
   "source": [
    "for slang in slang_words:\n",
    "    token_index = tokenizer.convert_tokens_to_ids(slang)\n",
    "    if token_index != tokenizer.unk_token_id:\n",
    "        print(f\"'{slang}' successfully added to the tokenizer with index {token_index}.\")\n",
    "    else:\n",
    "        print(f\"'{slang}' was not added to the tokenizer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'lol': tensor([-0.0323, -0.4869, -0.0764,  0.5014, -0.4434])\n",
      "Embedding for 'lmao': tensor([-0.1084, -0.4460, -0.1522,  0.3065, -0.4368])\n",
      "Embedding for 'omg': tensor([ 0.0654, -0.4869,  0.0401,  0.2163, -0.0247])\n",
      "Embedding for 'wtf': tensor([ 0.0321, -0.6249, -0.0311,  0.2451, -0.1961])\n"
     ]
    }
   ],
   "source": [
    "for slang in slang_words:\n",
    "    token_index = tokenizer.convert_tokens_to_ids(slang)\n",
    "    if token_index != tokenizer.unk_token_id:\n",
    "        embedding_vector = embedding_layer.weight.data[token_index]\n",
    "        print(f\"Embedding for '{slang}': {embedding_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'lol': Initialized with GloVe? Yes\n",
      "'lmao': Initialized with GloVe? Yes\n",
      "'omg': Initialized with GloVe? Yes\n",
      "'wtf': Initialized with GloVe? Yes\n"
     ]
    }
   ],
   "source": [
    "for slang in slang_words:\n",
    "    token_index = tokenizer.convert_tokens_to_ids(slang)\n",
    "    if slang in glove_embeddings and token_index != tokenizer.unk_token_id:\n",
    "        glove_vector = torch.tensor(glove_embeddings[slang], dtype=torch.float32)\n",
    "        projected_vector = projection_layer(glove_vector.unsqueeze(0)).squeeze(0)\n",
    "        bert_embedding = embedding_layer.weight.data[token_index]\n",
    "        is_close = torch.allclose(bert_embedding, projected_vector, atol=1e-3)\n",
    "        print(f\"'{slang}': Initialized with GloVe? {'Yes' if is_close else 'No'}\")\n",
    "\n",
    "for slang in slang_words:\n",
    "    token_index = tokenizer.convert_tokens_to_ids(slang)\n",
    "    if slang not in glove_embeddings and token_index != tokenizer.unk_token_id:\n",
    "        embedding_vector = embedding_layer.weight.data[token_index]\n",
    "        print(f\"'{slang}' was randomly initialized with values: {embedding_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserting emoji in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import keyedvectors\n",
    "\n",
    "e2v = keyedvectors.load_word2vec_format('Embeddings/emoji2vec.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of e2v embedding: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dimension of e2v embedding:\", e2v[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "E2V_EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_list = [\"üòÄ\", \"üòÇ\", \"‚ù§Ô∏è\", \"üî•\", \"üëç\", \"üò≠\", \"üôè\", \"ü•∫\", \"üòç\", \"üòÖ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 'üòÄ' with projected e2v weights.\n",
      "Initialized 'üòÇ' with projected e2v weights.\n",
      "Initialized '‚ù§Ô∏è' with projected e2v weights.\n",
      "Initialized 'üî•' with projected e2v weights.\n",
      "Initialized 'üëç' with projected e2v weights.\n",
      "Initialized 'üò≠' with projected e2v weights.\n",
      "Initialized 'üôè' with projected e2v weights.\n",
      "Initialized 'ü•∫' with random weights.\n",
      "Initialized 'üòç' with projected e2v weights.\n",
      "Initialized 'üòÖ' with projected e2v weights.\n"
     ]
    }
   ],
   "source": [
    "new_emoji_tokens = [emoji for emoji in emoji_list if emoji not in tokenizer.get_vocab()]\n",
    "tokenizer.add_tokens(new_emoji_tokens)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "embedding_layer = model.bert.embeddings.word_embeddings\n",
    "\n",
    "emoji_projection_layer = torch.nn.Linear(E2V_EMBEDDING_SIZE, model.config.hidden_size)\n",
    "\n",
    "for emoji in new_emoji_tokens:\n",
    "    token_index = tokenizer.convert_tokens_to_ids(emoji)\n",
    "    \n",
    "    if emoji in e2v:\n",
    "        e2v_vector = torch.tensor(e2v[emoji], dtype=torch.float32)\n",
    "        projected_vector = emoji_projection_layer(e2v_vector.unsqueeze(0)).squeeze(0)\n",
    "        embedding_layer.weight.data[token_index] = projected_vector\n",
    "        print(f\"Initialized '{emoji}' with projected e2v weights.\")\n",
    "    else:\n",
    "        embedding_layer.weight.data[token_index] = torch.randn(model.config.hidden_size)\n",
    "        print(f\"Initialized '{emoji}' with random weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'üòÄ' initialized with Emoji2Vec weights.\n",
      "'üòÇ' initialized with Emoji2Vec weights.\n",
      "'‚ù§Ô∏è' initialized with Emoji2Vec weights.\n",
      "'üî•' initialized with Emoji2Vec weights.\n",
      "'üëç' initialized with Emoji2Vec weights.\n",
      "'üò≠' initialized with Emoji2Vec weights.\n",
      "'üôè' initialized with Emoji2Vec weights.\n",
      "'ü•∫' initialized with random weights.\n",
      "'üòç' initialized with Emoji2Vec weights.\n",
      "'üòÖ' initialized with Emoji2Vec weights.\n"
     ]
    }
   ],
   "source": [
    "for emoji in emoji_list:\n",
    "    token_index = tokenizer.convert_tokens_to_ids(emoji)\n",
    "    if emoji in e2v.key_to_index:\n",
    "        print(f\"'{emoji}' initialized with Emoji2Vec weights.\")\n",
    "    else:\n",
    "        print(f\"'{emoji}' initialized with random weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'üòÄ': tensor([ 0.0116, -0.0169,  0.0395, -0.0076,  0.0237])\n",
      "Embedding for 'üòÇ': tensor([ 0.0213,  0.0384,  0.0585, -0.0085, -0.0127])\n",
      "Embedding for '‚ù§Ô∏è': tensor([ 0.0173,  0.0480,  0.0604, -0.0098,  0.0586])\n",
      "Embedding for 'üî•': tensor([ 0.0571,  0.0239,  0.0397, -0.0350,  0.0097])\n",
      "Embedding for 'üëç': tensor([ 0.0179,  0.0478,  0.0260, -0.0198,  0.0233])\n",
      "Embedding for 'üò≠': tensor([ 0.0044,  0.0139,  0.0420, -0.0062, -0.0175])\n",
      "Embedding for 'üôè': tensor([-0.0031,  0.0013,  0.0230, -0.0215, -0.0086])\n",
      "Embedding for 'ü•∫': tensor([-0.0629,  1.5145,  0.4056,  0.2759,  1.2953])\n",
      "Embedding for 'üòç': tensor([ 0.0656,  0.0093,  0.0686, -0.0363,  0.0171])\n",
      "Embedding for 'üòÖ': tensor([-0.0297,  0.0415,  0.0760, -0.0487, -0.0274])\n"
     ]
    }
   ],
   "source": [
    "for emoji in emoji_list:\n",
    "    token_index = tokenizer.convert_tokens_to_ids(emoji)\n",
    "    embedding_vector = embedding_layer.weight.data[token_index]\n",
    "    print(f\"Embedding for '{emoji}': {embedding_vector[:5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
