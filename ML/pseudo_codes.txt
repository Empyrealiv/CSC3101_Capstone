CLASS WHLA_BERT(Neural Network Model):
    FUNCTION Initialize(pretrained_model="bert-base-uncased", num_labels=2):
        Initialize parent class
        
        bert = Load BertModel from pretrained_model with output_hidden_states=True
        hidden_size = bert.config.hidden_size
        
        weights = Create learnable Parameter tensor of ones with size 4
        fully connected layer = Map a linear layer from hidden_size to num_labels
        dropout = Create Dropout layer with rate 0.5
        layer_norm = Create a layer normalization layer for hidden_size
    
    FUNCTION Forward_Propagation(input_ids, attention_mask, labels=None):
        outputs = Pass input_ids and attention_mask into the bert
        layers = Get all layers from outputs
        
        L9 = Get 4th-to-last layer output
        L10 = Get 3rd-to-last layer output
        L11 = Get 2nd-to-last layer output
        L12 = Get last layer output
        
        weighted_sum = Multiply each layer output by its corresponding weight value and sum them together
                      (weights[0] * L9 + weights[1] * L10 + weights[2] * L11 + weights[3] * L12)
        
        normalized_sum = Apply layer normalization to weighted_sum
        cls_representation = Extract the first token ([CLS]) representation from normalized_sum
        
        logits = Apply dropout to cls_representation, then pass through the 
                 fully connected layer to obtain the logits output
        
        IF labels is provided:
            loss_function = Create CrossEntropyLoss
            loss = Calculate loss between logits and labels
            RETURN dictionary with loss and logits
        ELSE:
            RETURN logits

FUNCTION Split_Data_For_Pretraining_And_Finetuning:
    INPUT:
        preview_data: DataFrame containing the data
        pretrain_percentage: Percentage of data to use for pretraining (0.70)
        threshold_difference: Maximum acceptable difference in class counts (7000)
        RANDOM_SEED: Random seed for reproducibility
        
    positive_data = Filter preview_data where for positive labelled data
    negative_data = Filter preview_data where for negative labelled data
    
    Initialize empty list for pretraining_samples and finetuning_samples
    
    FOR each unique source in preview_data:
        positive_total = Filter positive_data where data source matches current source
        negative_total = Filter negative_data where data source matches current source
        
        # Count samples per class
        positive_count = Count of positive_total
        negative_count = Count of negative_total
        difference = Absolute difference between positive_count and negative_count
        
        IF difference < threshold_difference:
            # Case 1: Counts are closely balanced, perform 70:30 split
            positive_pretrain = Random sample 70% from positive_total
            negative_pretrain = Random sample 70% from negative_total
            positive_finetune = positive_total minus positive_pretrain
            negative_finetune = negative_total minus negative_pretrain
            
        ELSE:
            # Case 2: Counts are imbalanced
            # First, get the smaller count
            min_count = Minimum between positive_count and negative_count
            
            # Create balanced dataset for fine-tuning
            positive_finetune = Random sample min_count from positive_total
            negative_finetune = Random sample min_count from negative_total
            
            # Send remaining samples to pretraining
            positive_pretrain = positive_total minus positive_finetune
            negative_pretrain = negative_total minus negative_finetune
        
        # Add samples to respective lists
        Add positive_pretrain to pretraining_samples
        Add negative_pretrain to pretraining_samples
        Add positive_finetune to finetuning_samples
        Add negative_finetune to finetuning_samples
    
    # Combine all samples into final respective datasets
    pretraining_data = Concatenate all pretraining_samples
    finetuning_data = Concatenate all finetuning_samples
    
    pretraining_data = Shuffle pretraining_data with RANDOM_SEED and reset indices
    finetuning_data = Shuffle finetuning_data with RANDOM_SEED and reset indices
    
    RETURN pretraining_data, finetuning_data

FUNCTION Vocab_Expansion:
INPUT:
    token_list: The list of tokens to be added into the model and tokenizer
    STATIC_EMBEDDING_SIZE: The embedding size of the static embeddings used
    static_embeddings: A dictionary containing weight embeddings for different tokens

    Initialize the bert model and the tokenizer

    projection_layer = Create a torch linear layer to project STATIC_EMBEDDING_SIZE
                        to the model embedding size

    Intialize empty list for new_token_list
    
    FOR each token in token_list:
        If token does exist in tokenizer vocabulary:
            Add token to new_token_list

    Add new_token_list to the tokenizer
    Resize the model token embeddings to the lenght of the tokenizer

    For token in new_token_list:
        If token exist in static_embeddings:
            vector = Obtain the weight embeddings of the token from the static embeddings
            projected_vector = Apply projection_layer to the vector
            Assign the weight embeddings of the token in the model to the projected_vector

        Else:
            Assign randoms weight embeddings to the token in the model

    RETURN model and tokenizer